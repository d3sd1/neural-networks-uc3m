Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 32, 32, 3)        12        
 ormalization)                                                   
                                                                 
 conv2d (Conv2D)             (None, 32, 32, 16)        448       
                                                                 
 dropout (Dropout)           (None, 32, 32, 16)        0         
                                                                 
 max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 16, 16, 32)        4640      
                                                                 
 dropout_1 (Dropout)         (None, 16, 16, 32)        0         
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 2048)              0         
                                                                 
 dense (Dense)               (None, 32)                65568     
                                                                 
 dropout_2 (Dropout)         (None, 32)                0         
                                                                 
 dense_1 (Dense)             (None, 16)                528       
                                                                 
 dropout_3 (Dropout)         (None, 16)                0         
                                                                 
 dense_2 (Dense)             (None, 10)                170       
                                                                 
=================================================================
Total params: 71,366
Trainable params: 71,360
Non-trainable params: 6
_________________________________________________________________
Epoch 1/30
1563/1563 [==============================] - 35s 22ms/step - loss: 1.9160 - sparse_categorical_accuracy: 0.2681 - val_loss: 1.6148 - val_sparse_categorical_accuracy: 0.4388
Epoch 2/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.6222 - sparse_categorical_accuracy: 0.3914 - val_loss: 1.4548 - val_sparse_categorical_accuracy: 0.5116
Epoch 3/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.4943 - sparse_categorical_accuracy: 0.4525 - val_loss: 1.3334 - val_sparse_categorical_accuracy: 0.5636
Epoch 4/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.4059 - sparse_categorical_accuracy: 0.4867 - val_loss: 1.2434 - val_sparse_categorical_accuracy: 0.5818
Epoch 5/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.3627 - sparse_categorical_accuracy: 0.5048 - val_loss: 1.2446 - val_sparse_categorical_accuracy: 0.5893
Epoch 6/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.3247 - sparse_categorical_accuracy: 0.5188 - val_loss: 1.1898 - val_sparse_categorical_accuracy: 0.6104
Epoch 7/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.3019 - sparse_categorical_accuracy: 0.5264 - val_loss: 1.1766 - val_sparse_categorical_accuracy: 0.6090
Epoch 8/30
1563/1563 [==============================] - 33s 21ms/step - loss: 1.2855 - sparse_categorical_accuracy: 0.5327 - val_loss: 1.1246 - val_sparse_categorical_accuracy: 0.6234
Epoch 9/30
1563/1563 [==============================] - 33s 21ms/step - loss: 1.2692 - sparse_categorical_accuracy: 0.5405 - val_loss: 1.1310 - val_sparse_categorical_accuracy: 0.6232
Epoch 10/30
1563/1563 [==============================] - 33s 21ms/step - loss: 1.2622 - sparse_categorical_accuracy: 0.5416 - val_loss: 1.1602 - val_sparse_categorical_accuracy: 0.6149
Epoch 11/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2532 - sparse_categorical_accuracy: 0.5438 - val_loss: 1.1384 - val_sparse_categorical_accuracy: 0.6181
Epoch 12/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2413 - sparse_categorical_accuracy: 0.5501 - val_loss: 1.1216 - val_sparse_categorical_accuracy: 0.6228
Epoch 13/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2230 - sparse_categorical_accuracy: 0.5534 - val_loss: 1.1011 - val_sparse_categorical_accuracy: 0.6400
Epoch 14/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2227 - sparse_categorical_accuracy: 0.5567 - val_loss: 1.0738 - val_sparse_categorical_accuracy: 0.6403
Epoch 15/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2099 - sparse_categorical_accuracy: 0.5609 - val_loss: 1.0976 - val_sparse_categorical_accuracy: 0.6318
Epoch 16/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2085 - sparse_categorical_accuracy: 0.5610 - val_loss: 1.1031 - val_sparse_categorical_accuracy: 0.6396
Epoch 17/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.2047 - sparse_categorical_accuracy: 0.5633 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.6328
Epoch 18/30
1563/1563 [==============================] - 34s 21ms/step - loss: 1.1906 - sparse_categorical_accuracy: 0.5672 - val_loss: 1.1059 - val_sparse_categorical_accuracy: 0.6290
Epoch 19/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1863 - sparse_categorical_accuracy: 0.5683 - val_loss: 1.0615 - val_sparse_categorical_accuracy: 0.6434
Epoch 20/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1759 - sparse_categorical_accuracy: 0.5711 - val_loss: 1.0675 - val_sparse_categorical_accuracy: 0.6367
Epoch 21/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1829 - sparse_categorical_accuracy: 0.5712 - val_loss: 1.0448 - val_sparse_categorical_accuracy: 0.6547
Epoch 22/30
1563/1563 [==============================] - 34s 21ms/step - loss: 1.1732 - sparse_categorical_accuracy: 0.5699 - val_loss: 1.0504 - val_sparse_categorical_accuracy: 0.6481
Epoch 23/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1742 - sparse_categorical_accuracy: 0.5711 - val_loss: 1.0397 - val_sparse_categorical_accuracy: 0.6465
Epoch 24/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1632 - sparse_categorical_accuracy: 0.5743 - val_loss: 1.0501 - val_sparse_categorical_accuracy: 0.6547
Epoch 25/30
1563/1563 [==============================] - 35s 22ms/step - loss: 1.1644 - sparse_categorical_accuracy: 0.5773 - val_loss: 1.0638 - val_sparse_categorical_accuracy: 0.6404
Epoch 26/30
1563/1563 [==============================] - 35s 22ms/step - loss: 1.1632 - sparse_categorical_accuracy: 0.5786 - val_loss: 1.0414 - val_sparse_categorical_accuracy: 0.6567
Epoch 27/30
1563/1563 [==============================] - 35s 22ms/step - loss: 1.1574 - sparse_categorical_accuracy: 0.5798 - val_loss: 1.0379 - val_sparse_categorical_accuracy: 0.6554
Epoch 28/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1446 - sparse_categorical_accuracy: 0.5809 - val_loss: 1.0290 - val_sparse_categorical_accuracy: 0.6517
Epoch 29/30
1563/1563 [==============================] - 34s 22ms/step - loss: 1.1471 - sparse_categorical_accuracy: 0.5811 - val_loss: 1.0201 - val_sparse_categorical_accuracy: 0.6638
Epoch 30/30
1563/1563 [==============================] - 34s 21ms/step - loss: 1.1495 - sparse_categorical_accuracy: 0.5812 - val_loss: 1.0442 - val_sparse_categorical_accuracy: 0.6557
313/313 [==============================] - 2s 5ms/step - loss: 1.0442 - sparse_categorical_accuracy: 0.6557
[[3.7452839e-03 1.8957517e-03 5.9813574e-02 3.5938057e-01 2.8044969e-02
  4.2675167e-01 5.8933306e-02 5.0479464e-02 7.0872279e-03 3.8682036e-03]
 [1.4717574e-01 4.4033457e-02 2.8001601e-04 1.9680414e-05 2.3308567e-05
  3.3275069e-07 3.2373998e-06 3.1453797e-07 8.0383474e-01 4.6291715e-03]
 [2.4351402e-01 1.6348688e-01 8.9285284e-02 8.8056073e-02 3.5139542e-02
  2.9826699e-02 1.4722191e-02 1.3960488e-02 2.3920432e-01 8.2804434e-02]
 [3.6939615e-01 5.5471268e-02 1.5471888e-01 5.3190444e-02 5.0129190e-02
  1.4278896e-02 2.1377735e-02 8.9716315e-03 2.1970111e-01 5.2764747e-02]
 [2.6403950e-05 5.7755798e-05 1.6523242e-02 2.2822976e-02 1.9113375e-02
  3.5884262e-03 9.3782955e-01 3.5139237e-05 1.7010309e-06 1.3724608e-06]]
[5 8 0 0 6 6 1 4 3 1 0 9 5 7 9 2 5 5 8 6]
[3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]
[[756   7  80  31  16  18   5  10  54  23]
 [ 60 718   1  39   3  12  12   4  25 126]
 [ 79   0 514  79 112 102  86  24   1   3]
 [  8   2  76 218  69 515  83  18   5   6]
 [ 22   0 101  50 614  56 103  50   3   1]
 [  9   1  47 161  50 664  22  39   3   4]
 [  6   0  53  74  21  32 810   1   2   1]
 [ 11   0  30  35  95 127   3 694   2   3]
 [ 92  20  14  36   6  14   3   6 792  17]
 [ 33  51   7  31   7  34   4  23  33 777]]
