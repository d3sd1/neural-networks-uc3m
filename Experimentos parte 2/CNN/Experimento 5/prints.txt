Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 32, 32, 3)        12        
 ormalization)                                                   
                                                                 
 conv2d (Conv2D)             (None, 32, 32, 32)        896       
                                                                 
 dropout (Dropout)           (None, 32, 32, 32)        0         
                                                                 
 max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     
                                                                 
 dropout_1 (Dropout)         (None, 16, 16, 64)        0         
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     
                                                                 
 dropout_2 (Dropout)         (None, 8, 8, 128)         0         
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 2048)              0         
                                                                 
 dense (Dense)               (None, 128)               262272    
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 356,822
Trainable params: 356,816
Non-trainable params: 6
_________________________________________________________________
Epoch 1/30
1563/1563 [==============================] - 74s 47ms/step - loss: 1.5448 - sparse_categorical_accuracy: 0.4410 - val_loss: 1.3482 - val_sparse_categorical_accuracy: 0.5970
Epoch 2/30
1563/1563 [==============================] - 69s 44ms/step - loss: 1.1687 - sparse_categorical_accuracy: 0.5888 - val_loss: 1.0694 - val_sparse_categorical_accuracy: 0.6720
Epoch 3/30
1563/1563 [==============================] - 70s 45ms/step - loss: 1.0289 - sparse_categorical_accuracy: 0.6413 - val_loss: 0.9673 - val_sparse_categorical_accuracy: 0.6885
Epoch 4/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.9517 - sparse_categorical_accuracy: 0.6690 - val_loss: 0.9298 - val_sparse_categorical_accuracy: 0.7072
Epoch 5/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.8950 - sparse_categorical_accuracy: 0.6913 - val_loss: 0.9089 - val_sparse_categorical_accuracy: 0.7214
Epoch 6/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.8505 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.8568 - val_sparse_categorical_accuracy: 0.7308
Epoch 7/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.8117 - sparse_categorical_accuracy: 0.7194 - val_loss: 0.8161 - val_sparse_categorical_accuracy: 0.7405
Epoch 8/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.7828 - sparse_categorical_accuracy: 0.7284 - val_loss: 0.8058 - val_sparse_categorical_accuracy: 0.7371
Epoch 9/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.7645 - sparse_categorical_accuracy: 0.7357 - val_loss: 0.7672 - val_sparse_categorical_accuracy: 0.7526
Epoch 10/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.7368 - sparse_categorical_accuracy: 0.7446 - val_loss: 0.7149 - val_sparse_categorical_accuracy: 0.7629
Epoch 11/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.7160 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.7445 - val_sparse_categorical_accuracy: 0.7586
Epoch 12/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6974 - sparse_categorical_accuracy: 0.7581 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7690
Epoch 13/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6824 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.7085 - val_sparse_categorical_accuracy: 0.7666
Epoch 14/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6668 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.7401 - val_sparse_categorical_accuracy: 0.7550
Epoch 15/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.6566 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.7088 - val_sparse_categorical_accuracy: 0.7666
Epoch 16/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6462 - sparse_categorical_accuracy: 0.7757 - val_loss: 0.6885 - val_sparse_categorical_accuracy: 0.7768
Epoch 17/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6385 - sparse_categorical_accuracy: 0.7797 - val_loss: 0.6968 - val_sparse_categorical_accuracy: 0.7702
Epoch 18/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.6255 - sparse_categorical_accuracy: 0.7827 - val_loss: 0.7199 - val_sparse_categorical_accuracy: 0.7596
Epoch 19/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.6065 - sparse_categorical_accuracy: 0.7887 - val_loss: 0.6819 - val_sparse_categorical_accuracy: 0.7772
Epoch 20/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6077 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.6957 - val_sparse_categorical_accuracy: 0.7679
Epoch 21/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.6004 - sparse_categorical_accuracy: 0.7918 - val_loss: 0.6677 - val_sparse_categorical_accuracy: 0.7753
Epoch 22/30
1563/1563 [==============================] - 71s 46ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.6815 - val_sparse_categorical_accuracy: 0.7763
Epoch 23/30
1563/1563 [==============================] - 70s 45ms/step - loss: 0.5771 - sparse_categorical_accuracy: 0.8003 - val_loss: 0.6588 - val_sparse_categorical_accuracy: 0.7845
Epoch 24/30
1563/1563 [==============================] - 72s 46ms/step - loss: 0.5665 - sparse_categorical_accuracy: 0.8017 - val_loss: 0.6502 - val_sparse_categorical_accuracy: 0.7909
Epoch 25/30
1563/1563 [==============================] - 72s 46ms/step - loss: 0.5605 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.6505 - val_sparse_categorical_accuracy: 0.7865
Epoch 26/30
1563/1563 [==============================] - 69s 44ms/step - loss: 0.5563 - sparse_categorical_accuracy: 0.8070 - val_loss: 0.6531 - val_sparse_categorical_accuracy: 0.7826
Epoch 27/30
1563/1563 [==============================] - 72s 46ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.8063 - val_loss: 0.6410 - val_sparse_categorical_accuracy: 0.7822
Epoch 28/30
1563/1563 [==============================] - 71s 45ms/step - loss: 0.5535 - sparse_categorical_accuracy: 0.8073 - val_loss: 0.6583 - val_sparse_categorical_accuracy: 0.7795
Epoch 29/30
1563/1563 [==============================] - 71s 46ms/step - loss: 0.5343 - sparse_categorical_accuracy: 0.8134 - val_loss: 0.6496 - val_sparse_categorical_accuracy: 0.7852
Epoch 30/30
1563/1563 [==============================] - 69s 44ms/step - loss: 0.5334 - sparse_categorical_accuracy: 0.8132 - val_loss: 0.6275 - val_sparse_categorical_accuracy: 0.7891
313/313 [==============================] - 3s 10ms/step - loss: 0.6275 - sparse_categorical_accuracy: 0.7891
[[8.1469421e-05 4.5731547e-05 4.6503489e-04 5.4077631e-01 1.0831158e-04
  4.5679140e-01 7.6674763e-04 8.6310547e-04 7.7390410e-05 2.4415434e-05]
 [1.6246150e-03 3.4799161e-03 6.1171892e-07 1.1343230e-06 2.3502585e-07
  1.6098359e-07 1.9923827e-06 1.7103185e-08 9.9480599e-01 8.5444146e-05]
 [2.0093066e-01 1.0506985e-01 3.5650063e-02 5.5695340e-02 3.1371079e-02
  2.7471635e-02 2.1451017e-02 1.7401598e-02 4.0095454e-01 1.0400422e-01]
 [4.6778840e-01 1.8024197e-02 3.3713784e-02 9.2205368e-03 1.7678283e-02
  4.1261907e-03 9.0313843e-03 2.7958879e-03 3.7918687e-01 5.8434445e-02]
 [2.2873386e-05 3.2830954e-05 2.6774893e-02 3.9896802e-03 3.0584747e-03
  6.4437627e-05 9.6588171e-01 2.4154479e-05 1.3377775e-04 1.7135901e-05]]
[3 8 8 0 6 6 1 4 3 1 4 9 5 7 9 6 5 7 8 6]
[3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]
[[815   3  42  22  38   1   8  10  50  11]
 [ 21 840   4   8   5   3  12   7  36  64]
 [ 40   0 675  55 123  40  47  14   6   0]
 [ 12   2  41 657 110  73  69  24   7   5]
 [  5   0  32  29 860  10  26  33   4   1]
 [  8   1  23 216  85 600  19  42   5   1]
 [  3   0  31  51  40   5 865   1   4   0]
 [  6   0  21  21 102  28   5 816   1   0]
 [ 31   6   5   8  10   1   6   3 922   8]
 [ 38  34   4  16  21   2   7  15  22 841]]
