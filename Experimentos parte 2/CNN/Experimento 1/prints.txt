Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 32, 32, 3)        12        
 ormalization)                                                   
                                                                 
 conv2d (Conv2D)             (None, 32, 32, 16)        448       
                                                                 
 dropout (Dropout)           (None, 32, 32, 16)        0         
                                                                 
 max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         
 )                                                               
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 32)                131104    
                                                                 
 dropout_1 (Dropout)         (None, 32)                0         
                                                                 
 dense_1 (Dense)             (None, 10)                330       
                                                                 
=================================================================
Total params: 131,894
Trainable params: 131,888
Non-trainable params: 6
_________________________________________________________________
Epoch 1/30
1563/1563 [==============================] - 24s 15ms/step - loss: 2.0251 - sparse_categorical_accuracy: 0.1969 - val_loss: 1.7684 - val_sparse_categorical_accuracy: 0.3169
Epoch 2/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.8613 - sparse_categorical_accuracy: 0.2660 - val_loss: 1.7118 - val_sparse_categorical_accuracy: 0.3571
Epoch 3/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.7943 - sparse_categorical_accuracy: 0.3072 - val_loss: 1.5794 - val_sparse_categorical_accuracy: 0.4292
Epoch 4/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.7134 - sparse_categorical_accuracy: 0.3453 - val_loss: 1.5038 - val_sparse_categorical_accuracy: 0.4574
Epoch 5/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.6821 - sparse_categorical_accuracy: 0.3610 - val_loss: 1.5458 - val_sparse_categorical_accuracy: 0.4484
Epoch 6/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.6561 - sparse_categorical_accuracy: 0.3743 - val_loss: 1.4784 - val_sparse_categorical_accuracy: 0.4795
Epoch 7/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.5946 - sparse_categorical_accuracy: 0.4065 - val_loss: 1.3781 - val_sparse_categorical_accuracy: 0.5233
Epoch 8/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.5458 - sparse_categorical_accuracy: 0.4282 - val_loss: 1.3794 - val_sparse_categorical_accuracy: 0.5203
Epoch 9/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.5115 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.3247 - val_sparse_categorical_accuracy: 0.5396
Epoch 10/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4907 - sparse_categorical_accuracy: 0.4438 - val_loss: 1.3249 - val_sparse_categorical_accuracy: 0.5447
Epoch 11/30
1563/1563 [==============================] - 23s 14ms/step - loss: 1.4801 - sparse_categorical_accuracy: 0.4476 - val_loss: 1.3256 - val_sparse_categorical_accuracy: 0.5477
Epoch 12/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4633 - sparse_categorical_accuracy: 0.4550 - val_loss: 1.3233 - val_sparse_categorical_accuracy: 0.5467
Epoch 13/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4367 - sparse_categorical_accuracy: 0.4628 - val_loss: 1.2825 - val_sparse_categorical_accuracy: 0.5652
Epoch 14/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4195 - sparse_categorical_accuracy: 0.4689 - val_loss: 1.2858 - val_sparse_categorical_accuracy: 0.5601
Epoch 15/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4063 - sparse_categorical_accuracy: 0.4741 - val_loss: 1.2612 - val_sparse_categorical_accuracy: 0.5723
Epoch 16/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.4009 - sparse_categorical_accuracy: 0.4780 - val_loss: 1.2555 - val_sparse_categorical_accuracy: 0.5650
Epoch 17/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.3901 - sparse_categorical_accuracy: 0.4822 - val_loss: 1.2458 - val_sparse_categorical_accuracy: 0.5701
Epoch 18/30
1563/1563 [==============================] - 23s 14ms/step - loss: 1.3506 - sparse_categorical_accuracy: 0.4942 - val_loss: 1.2118 - val_sparse_categorical_accuracy: 0.5685
Epoch 19/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.3407 - sparse_categorical_accuracy: 0.5007 - val_loss: 1.2023 - val_sparse_categorical_accuracy: 0.5838
Epoch 20/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.3264 - sparse_categorical_accuracy: 0.5055 - val_loss: 1.2157 - val_sparse_categorical_accuracy: 0.5800
Epoch 21/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.3207 - sparse_categorical_accuracy: 0.5056 - val_loss: 1.2267 - val_sparse_categorical_accuracy: 0.5736
Epoch 22/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.3156 - sparse_categorical_accuracy: 0.5127 - val_loss: 1.1945 - val_sparse_categorical_accuracy: 0.5886
Epoch 23/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.3140 - sparse_categorical_accuracy: 0.5151 - val_loss: 1.2087 - val_sparse_categorical_accuracy: 0.5715
Epoch 24/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.3016 - sparse_categorical_accuracy: 0.5150 - val_loss: 1.1876 - val_sparse_categorical_accuracy: 0.5877
Epoch 25/30
1563/1563 [==============================] - 23s 14ms/step - loss: 1.3026 - sparse_categorical_accuracy: 0.5144 - val_loss: 1.2107 - val_sparse_categorical_accuracy: 0.5816
Epoch 26/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.3004 - sparse_categorical_accuracy: 0.5153 - val_loss: 1.2056 - val_sparse_categorical_accuracy: 0.5803
Epoch 27/30
1563/1563 [==============================] - 23s 15ms/step - loss: 1.2895 - sparse_categorical_accuracy: 0.5191 - val_loss: 1.2053 - val_sparse_categorical_accuracy: 0.5832
Epoch 28/30
1563/1563 [==============================] - 23s 14ms/step - loss: 1.2855 - sparse_categorical_accuracy: 0.5227 - val_loss: 1.2062 - val_sparse_categorical_accuracy: 0.5812
Epoch 29/30
1563/1563 [==============================] - 23s 14ms/step - loss: 1.2817 - sparse_categorical_accuracy: 0.5223 - val_loss: 1.1838 - val_sparse_categorical_accuracy: 0.5928
Epoch 30/30
1563/1563 [==============================] - 22s 14ms/step - loss: 1.2875 - sparse_categorical_accuracy: 0.5238 - val_loss: 1.1875 - val_sparse_categorical_accuracy: 0.5871
313/313 [==============================] - 1s 4ms/step - loss: 1.1875 - sparse_categorical_accuracy: 0.5871
[[2.04272917e-03 2.08540645e-04 1.07247785e-01 3.89764309e-01
  5.71758449e-02 2.46763021e-01 1.85620099e-01 3.31334537e-03
  7.66919786e-03 1.95100918e-04]
 [2.54695714e-02 4.92759287e-01 1.53700501e-06 6.08209257e-06
  1.83951457e-07 7.07837178e-10 5.76183936e-07 2.18264053e-11
  4.72971976e-01 8.79074819e-03]
 [4.00966287e-01 2.58297324e-02 2.25244984e-02 1.05079375e-02
  6.24269759e-03 4.05145867e-04 1.84025534e-03 6.23339292e-05
  5.18762827e-01 1.28582260e-02]
 [6.62155449e-01 9.52526752e-04 7.67107010e-02 6.48267241e-03
  1.48578957e-02 3.89292079e-04 2.26662401e-03 4.36758201e-05
  2.35249281e-01 8.91868258e-04]
 [1.10468412e-04 1.31089746e-05 4.25803773e-02 2.11474188e-02
  7.76341632e-02 1.88123249e-03 8.56398046e-01 2.23623778e-04
  2.09627206e-06 9.48933393e-06]]
[3 1 8 0 6 6 1 2 5 1 0 9 5 7 9 8 5 7 8 6]
[3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]
[[656  21 118  21  14   7   6  17  94  46]
 [ 31 710   7  36   4  10   9   6  67 120]
 [ 82   4 430  64 116 163  63  56   9  13]
 [ 11   8 123 217  70 406  74  65   7  19]
 [ 23   5 144  51 470  74  69 147  14   3]
 [  8   6  93 116  58 610  13  86   7   3]
 [  6   6  71  93  93  50 655  13   9   4]
 [ 15   1  30  20  67 122   5 723   0  17]
 [111  54  22  30   8   6   7   4 699  59]
 [ 38 109   7  28   4  13  10  41  49 701]]