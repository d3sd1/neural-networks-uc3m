Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 batch_normalization (BatchN  (None, 32, 32, 3)        12        
 ormalization)                                                   
                                                                 
 conv2d (Conv2D)             (None, 32, 32, 80)        11840     
                                                                 
 dropout (Dropout)           (None, 32, 32, 80)        0         
                                                                 
 max_pooling2d (MaxPooling2D  (None, 16, 16, 80)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 16, 16, 100)       72100     
                                                                 
 dropout_1 (Dropout)         (None, 16, 16, 100)       0         
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 8, 8, 100)        0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 6400)              0         
                                                                 
 dense (Dense)               (None, 50)                320050    
                                                                 
 dropout_2 (Dropout)         (None, 50)                0         
                                                                 
 dense_1 (Dense)             (None, 10)                510       
                                                                 
=================================================================
Total params: 404,512
Trainable params: 404,506
Non-trainable params: 6
_________________________________________________________________
Epoch 1/30
1563/1563 [==============================] - 126s 81ms/step - loss: 1.6481 - sparse_categorical_accuracy: 0.4005 - val_loss: 1.4291 - val_sparse_categorical_accuracy: 0.5276
Epoch 2/30
1563/1563 [==============================] - 191s 122ms/step - loss: 1.3444 - sparse_categorical_accuracy: 0.5185 - val_loss: 1.2303 - val_sparse_categorical_accuracy: 0.6044
Epoch 3/30
1563/1563 [==============================] - 197s 126ms/step - loss: 1.2193 - sparse_categorical_accuracy: 0.5636 - val_loss: 1.1173 - val_sparse_categorical_accuracy: 0.6266
Epoch 4/30
1563/1563 [==============================] - 213s 137ms/step - loss: 1.1203 - sparse_categorical_accuracy: 0.6027 - val_loss: 1.0509 - val_sparse_categorical_accuracy: 0.6529
Epoch 5/30
1563/1563 [==============================] - 197s 126ms/step - loss: 1.0472 - sparse_categorical_accuracy: 0.6283 - val_loss: 0.9968 - val_sparse_categorical_accuracy: 0.6746
Epoch 6/30
1563/1563 [==============================] - 191s 122ms/step - loss: 0.9924 - sparse_categorical_accuracy: 0.6493 - val_loss: 0.9852 - val_sparse_categorical_accuracy: 0.6769
Epoch 7/30
1563/1563 [==============================] - 180s 115ms/step - loss: 0.9378 - sparse_categorical_accuracy: 0.6683 - val_loss: 0.9474 - val_sparse_categorical_accuracy: 0.6865
Epoch 8/30
1563/1563 [==============================] - 181s 116ms/step - loss: 0.9014 - sparse_categorical_accuracy: 0.6808 - val_loss: 0.9104 - val_sparse_categorical_accuracy: 0.6934
Epoch 9/30
1563/1563 [==============================] - 184s 117ms/step - loss: 0.8551 - sparse_categorical_accuracy: 0.6973 - val_loss: 0.9031 - val_sparse_categorical_accuracy: 0.6973
Epoch 10/30
1563/1563 [==============================] - 125s 80ms/step - loss: 0.8249 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.8493 - val_sparse_categorical_accuracy: 0.7184
Epoch 11/30
1563/1563 [==============================] - 128s 82ms/step - loss: 0.7885 - sparse_categorical_accuracy: 0.7208 - val_loss: 0.8939 - val_sparse_categorical_accuracy: 0.7024
Epoch 12/30
1563/1563 [==============================] - 130s 83ms/step - loss: 0.7576 - sparse_categorical_accuracy: 0.7309 - val_loss: 0.8432 - val_sparse_categorical_accuracy: 0.7085
Epoch 13/30
1563/1563 [==============================] - 132s 84ms/step - loss: 0.7271 - sparse_categorical_accuracy: 0.7388 - val_loss: 0.8412 - val_sparse_categorical_accuracy: 0.7143
Epoch 14/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.7069 - sparse_categorical_accuracy: 0.7473 - val_loss: 0.8377 - val_sparse_categorical_accuracy: 0.7165
Epoch 15/30
1563/1563 [==============================] - 132s 85ms/step - loss: 0.6891 - sparse_categorical_accuracy: 0.7526 - val_loss: 0.8271 - val_sparse_categorical_accuracy: 0.7210
Epoch 16/30
1563/1563 [==============================] - 132s 84ms/step - loss: 0.6651 - sparse_categorical_accuracy: 0.7610 - val_loss: 0.8186 - val_sparse_categorical_accuracy: 0.7233
Epoch 17/30
1563/1563 [==============================] - 132s 84ms/step - loss: 0.6418 - sparse_categorical_accuracy: 0.7680 - val_loss: 0.8178 - val_sparse_categorical_accuracy: 0.7181
Epoch 18/30
1563/1563 [==============================] - 132s 85ms/step - loss: 0.6264 - sparse_categorical_accuracy: 0.7749 - val_loss: 0.8159 - val_sparse_categorical_accuracy: 0.7229
Epoch 19/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.6130 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.8062 - val_sparse_categorical_accuracy: 0.7265
Epoch 20/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.8084 - val_sparse_categorical_accuracy: 0.7245
Epoch 21/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5775 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.8197 - val_sparse_categorical_accuracy: 0.7212
Epoch 22/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.7938 - val_loss: 0.8126 - val_sparse_categorical_accuracy: 0.7293
Epoch 23/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5557 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.7951 - val_sparse_categorical_accuracy: 0.7300
Epoch 24/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.8096 - val_sparse_categorical_accuracy: 0.7279
Epoch 25/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5346 - sparse_categorical_accuracy: 0.8053 - val_loss: 0.8196 - val_sparse_categorical_accuracy: 0.7246
Epoch 26/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5198 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.8282 - val_sparse_categorical_accuracy: 0.7239
Epoch 27/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5120 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.8034 - val_sparse_categorical_accuracy: 0.7347
Epoch 28/30
1563/1563 [==============================] - 133s 85ms/step - loss: 0.5046 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.7954 - val_sparse_categorical_accuracy: 0.7347
Epoch 29/30
1563/1563 [==============================] - 132s 84ms/step - loss: 0.4942 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.8027 - val_sparse_categorical_accuracy: 0.7283
Epoch 30/30
1563/1563 [==============================] - 132s 84ms/step - loss: 0.4894 - sparse_categorical_accuracy: 0.8218 - val_loss: 0.8082 - val_sparse_categorical_accuracy: 0.7314
313/313 [==============================] - 5s 15ms/step - loss: 0.8082 - sparse_categorical_accuracy: 0.7314
[[1.1436742e-03 2.0963683e-04 4.5691966e-04 6.9658083e-01 6.3656666e-04
  2.9307410e-01 5.7659205e-03 3.9548645e-04 1.4247958e-03 3.1213628e-04]
 [1.2096493e-02 1.6331153e-02 3.3160998e-08 5.3251696e-09 1.3922369e-11
  5.1460436e-10 1.8376996e-08 3.1709143e-12 9.7155422e-01 1.8091681e-05]
 [3.0520886e-01 1.1479043e-01 1.9338763e-03 9.6277539e-03 2.8621138e-03
  8.6446985e-04 7.1913231e-04 6.4879464e-04 1.6955929e-01 3.9378530e-01]
 [9.5467621e-01 2.9851319e-04 8.1881002e-04 4.8945607e-05 8.3783387e-05
  4.1395310e-06 6.1623618e-06 1.0149559e-05 4.4023402e-02 2.9847803e-05]
 [1.2399932e-02 1.9733482e-03 5.9257329e-02 3.0458011e-02 7.9125291e-01
  1.1948605e-02 8.5282065e-02 2.7841821e-04 5.1451041e-03 2.0042204e-03]]
[3 8 9 0 4 6 1 2 3 1 0 9 5 7 9 8 5 7 8 6]
[3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]
[[799   9  40  20  23   2   4   9  60  34]
 [ 24 785   3  18   4   8   5   1  24 128]
 [ 61   3 560  85 121  68  45  41  12   4]
 [ 30   7  45 613  75 132  42  30  13  13]
 [ 14   2  36  81 729  22  31  72  11   2]
 [ 17   3  35 214  45 614  16  44   5   7]
 [ 13   4  44 112  59  20 730  11   3   4]
 [ 22   1  24  39  63  53   4 781   1  12]
 [ 60  23   6  15   7   6   5   7 844  27]
 [ 28  44   6  15   8   1   3  14  22 859]]
